{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Architecture1.png)\n",
    "![title](Architecture2.png)\n",
    "![title](Architecture3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Feedforward1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Backpropagation\n",
    "Step 1: Doing a feedforward operation.  \n",
    "Step 2: Comparing the output of the model with the desired output.  \n",
    "Step 3: Calculating the error.  \n",
    "Step 4: Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.  \n",
    "Step 5: Use this to update the weights, and get a better model.  \n",
    "Step 6: Continue this until we have a model that is good.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Backpropagation1.png)\n",
    "![title](Backpropagation2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "# f(x) = x**2 - 2*x + 1\n",
    "import numpy as np\n",
    "testF = lambda x: x**2 - 2*x + 1\n",
    "# f: function\n",
    "# init: 初始值\n",
    "# step: 学习率，步长\n",
    "# eps: 停止条件 精度\n",
    "def gd(f, init = 0, step = 0.1, eps = 0.0001):\n",
    "    deltaX = 0.01\n",
    "    devF = lambda x: (f(x) - f(x-deltaX))/deltaX\n",
    "    newpre = f(init)\n",
    "    error = np.inf\n",
    "    while error > eps:\n",
    "        init = init - devF(init)*step\n",
    "        error = abs(newpre - f(init))\n",
    "        newpre = f(init)\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9905164235983765"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd(testF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Outline key points for neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perceptron\n",
    "#### Input\n",
    "1. Onehot encode (for input)  \n",
    "pd.get_dummies(data)  \n",
    "sklearn.preprocessing.OneHotEncoder(sparse = False).fit_transform(data)\n",
    "2. Normallization (for input) min-max method: (x - min x)/(max x - min x)\n",
    "3. No correlation between features (optional)\n",
    "\n",
    "#### Output I: linear combination\n",
    "1. linear combination (for output)\n",
    "\n",
    "#### Output II: Activate function\n",
    "1. step function (discrete output)\n",
    "3. sigmoid function (continuous output)\n",
    "$$S(t)=\\frac{1}{1+e^{-t}}$$\n",
    "$$S'(t)=S(t)(1-S(t))$$\n",
    "6. softmax function (multi-classification problem)\n",
    "$$Softmax(x_i) = \\frac{e^{x_i}}{\\sum_i e^{x_i}}$$\n",
    "**reference: **http://blog.nex3z.com/2017/05/02/sigmoid-%E5%87%BD%E6%95%B0%E5%92%8C-softmax-%E5%87%BD%E6%95%B0%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E5%85%B3%E7%B3%BB/\n",
    "\n",
    "\n",
    "1. percrptron algo (adjustment)\n",
    "2. Maximum Likelihood (log transformation)\n",
    "3. cross entropy ~ $\\frac{1}{correct\\space probability}$  \n",
    "For 0-1 problem: \n",
    "$$Cross-Entropy(y, p) = -\\sum_{i=1}^m y'_iln(y_i) + (1-y'_i)ln(1-y_i)$$\n",
    "where $y_i$ is the predicted probability value for class i and $y'_i$ is the true probability for that class. m is data size.  \n",
    "For multi-class problem:\n",
    "$$Cross-Entropy = \\sum_{i=1}^m \\sum_{j=1}^n -y'^{(j)}_i ln(y^{(j)}_i)$$\n",
    "where n is classes\n",
    "4. Error Function\n",
    "$$E(w, b) = -\\frac{1}{m} \\sum_{i=1}^m y'_i ln(\\sigma(wx_i+b)) + (1-y'_i)ln(1-\\sigma(wx_i+b))$$\n",
    "5. Gradient Descent\n",
    "\n",
    "### Neural Network\n",
    "1. Feedforward Neural Network\n",
    "2. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***notes for onehot encoder***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "        x1        x2  y\n",
      "0  0.78051 -0.063669  1\n",
      "1  0.28774  0.291390  1\n",
      "        x1        x2  y_0  y_1\n",
      "0  0.78051 -0.063669    0    1\n",
      "1  0.28774  0.291390    0    1\n"
     ]
    }
   ],
   "source": [
    "# 1. Onehot\n",
    "# example 1:\n",
    "print('Example 1:')\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "data = pd.read_excel('data.xlsx')\n",
    "print(data.head(2))\n",
    "method_pd = pd.get_dummies(data, columns = ['y'])\n",
    "print(method_pd.head(2))\n",
    "sklearn_onehot = preprocessing.OneHotEncoder(sparse = False)\n",
    "method_sklearn = sklearn_onehot.fit_transform(data['y'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2:\n",
      "[[1. 0. 0. 1. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# example 2:\n",
    "print('Example 2:')\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit([[0, 0, 3],\n",
    "         [1, 1, 0],\n",
    "         [0, 2, 1],\n",
    "         [1, 0, 2]])\n",
    "# toarray() return array, if not toarray, return sparse matrix\n",
    "# same effect with parameter: sparse = False\n",
    "ans = enc.transform([[0, 1, 2]]).toarray()\n",
    "# first feature: 0: 10, 1: 01\n",
    "# second feature: 0: 100, 1: 010, 2: 001\n",
    "# third feature: 0: 1000, 1: 0100, 2: 0010, 3: 0001\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***notes for normalization***  \n",
    "***reference:*** https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.03408479, -0.88028419]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit scaler\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit(data[['x1', 'x2']])\n",
    "X_train_minmax.transform(data[['x1', 'x2']])[:2]\n",
    "\n",
    "# new data input\n",
    "X_test = np.array([[-3., -1.]])\n",
    "X_test_minmax = X_train_minmax.transform(X_test)\n",
    "X_test_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625802</td>\n",
       "      <td>-0.767375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>0.625802</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.769329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-0.767375</td>\n",
       "      <td>-0.769329</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x1        x2         y\n",
       "x1  1.000000  0.625802 -0.767375\n",
       "x2  0.625802  1.000000 -0.769329\n",
       "y  -0.767375 -0.769329  1.000000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems when training neural network\n",
    "1. overfitting & underfitting  \n",
    "(1) Model complexity graph (determine epoch, early stopping)  \n",
    "(2) L1(feature selection), L2(use most) Regularization (Penalize for large weights)  \n",
    "(3) Dropout: randomly close some nodes in each epoch  \n",
    "prob each node will be dropout = 0.2 20% nodes will be turned off\n",
    "2. bias & variance\n",
    "3. gradient descent(local minimum, gradient disappear)  \n",
    "(1) change activate function(tanh, relu) for gradient disappear  \n",
    "(2) stochastic gradient descent (mini-batch, different dataset for each epoch)  \n",
    "(3) learning rate(learning rate decay strategy)  \n",
    "(4) random start point and run gradient descent to all points   \n",
    "(5) momentum $\\beta$  \n",
    "$$STEP(n)=STEP(n)+\\beta STEP(n-1)+\\beta^2STEP(n-2)+...$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](model_complex.png)\n",
    "![title](reg.png)\n",
    "![title](biasvar.png)\n",
    "![title](randstart.png)\n",
    "![title](mom.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. perceptron algorithm\n",
    "* step 1: start with random weights: w1, ..., wn, b  \n",
    "* step 2: for every misclassified point  \n",
    "```\n",
    "if prediction = 0:\n",
    "    for i in range(n)\n",
    "        change wi to wi + axi\n",
    "        change b to b + a\n",
    "where a is learning rate\n",
    "if prediction = 1:\n",
    "    for i in range(n)\n",
    "        change wi to wi - axi\n",
    "        change b to b - a```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i],W,b)\n",
    "        if y[i]-y_hat == 1:\n",
    "            W[0] += X[i][0]*learn_rate\n",
    "            W[1] += X[i][1]*learn_rate\n",
    "            b += learn_rate\n",
    "        elif y[i]-y_hat == -1:\n",
    "            W[0] -= X[i][0]*learn_rate\n",
    "            W[1] -= X[i][1]*learn_rate\n",
    "            b -= learn_rate\n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Descent\n",
    "* step 1: start with random weights: w1, ..., wn, b  \n",
    "* step 2: for every point $(x_1, x_2, ..., x_n)$  \n",
    "```\n",
    "For i in range(n):\n",
    "    Update wi to wi - a*partial(E)/partial(wi)\n",
    "    Update b to b - a*partial(E)/partial(b)\n",
    "```\n",
    "* Workshop: Implementing the Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Backpropagation\n",
    "* Workshop: Student Admissions\n",
    "* Reference:  \n",
    "(1) https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/  \n",
    "(2) http://neuralnetworksanddeeplearning.com/chap2.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
